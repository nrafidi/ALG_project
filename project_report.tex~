\documentclass{article}
\usepackage{geometry}
\usepackage{fancyhdr}
\usepackage{amsmath,amsthm,amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{enumerate}
\usepackage{caption}

\newcommand{\argmax}{\operatornamewithlimits{argmax}}

\title{Parallelizing Stochastic Gradient Descent}
\author{Kirstin Early, \qquad Nicole Rafidi}
%\date{\today}

\begin{document}
\maketitle
\section{Introduction}
\section{Related Work}
\section{Algorithms and Optimizations}
\subsection{Stochastic Gradient Descent}
In this project we implemented stochastic gradient descent for $L_2$ penalized logistic regression. The objective function we seek to minimize is:
\begin{equation}
\sum_{i = 1}^{N}(y_i - \hat{y}_i)^2 + \lambda||\vec{w}||_2^2
\end{equation}
Here $N$ is the number of samples, and we estimate $\hat{y} = sgn(w^Tx)$, where $x$ is the feature matrix (\in \mathbb{R}^{NxF} and $\vec{w} \in \mathbb{R}^F$ are the learned weights ($F$ is the number of features). The labels $y$ are assumed to take on values of either $1$ or $-1$. To update the weights via gradient descent, we use a logistic update equation:
\begin{equation}
w_f = w_f + \sum_{i = 1}^{N}(y_i - g(\vec{w}, x_i)) + \lambda w_f
\end{equation}
\begin{equation}
g(\vec{w}, x_i) = \frac{-w^Tx_ie^{-w^Tx_i}}{1 + e^{-w^Tx_i}
\end{equation}
\subsection{Parallelized Versions}
\subsection{Experiments}
\subsubsection{Data Sets}
\section{Results}
\section{Conclusions}

%\bibliographystyle{ieeetr}
%\bibliography{concats_ource}

\end{document}
