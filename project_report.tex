\documentclass{article}
\usepackage{geometry}
\usepackage{algorithmic}
%\usepackage{fancyhdr}
\usepackage{amsmath,amsthm,amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{enumerate}
\usepackage{caption}

\newcommand{\argmax}{\operatornamewithlimits{argmax}}

\title{Parallelizing Stochastic Gradient Descent}
\author{Kirstin Early \qquad Nicole Rafidi}
%\date{\today}

\begin{document}
\maketitle

\section{Introduction}
Gradient descent (also called coordinate descent) is a popular numerical technique for optimizing an expression that has no closed-form solution. This method can find a local minimum of a differentiable function by iteratively moving an initial guess of the minimum value in the opposite direction of the gradient at that point, either for a pre-specified number or iterations or until the update does not change the estimated value significantly. In the multivariate case, it is possible to update each coordinate individually in this manner. When data are numerous, stochastic gradient descent can improve efficiency by performing the update on a random subset of training examples or on a subset of features at each step.

In this project, we examine the performance improvement gained by parallelizing various sections of the stochastic gradient descent algorithm with OpenMP. We optimize the objective function for logistic regression with $L_2$ regularization:
\begin{equation}
\underset{w}{\min}-\frac{1}{1 + \exp(-y \cdot w^T x)} + \frac{\lambda}{2}||w||^2_2,
\end{equation}
where $x \in \mathbb{R}^F$ is a vector of sample features, $y \in \lbrace -1, 1 \rbrace$ is the sample label, and $w \in \mathbb{R}^F$ is a vector of feature weights. The first term in the optimization objective is the negative conditional likelihood of a sample's label given its features, $p(y|x)$; the second term is a regularization penalty on $w$. Sparse solutions (those solutions with few nonzero weights in $w$) are particularly desirable when the number of features exceeds the number of samples.

The optimum value for this objective function has no closed-form solution, and so we use gradient descent to find the minimum value. Letting $x \in \mathbb{R}^{N \times F}$ be the matrix of $N$ samples with $F$ features each, the update equation for a particular feature weight $w_f$ is
\begin{equation}
w_f = w_f + \eta \left( \sum_{i = 1}^{N}(y_i - g(\vec{w}, x_i)) + \lambda w_f \right) %Shouldn't we define this for w_f specifically? 
\end{equation}
\begin{equation}
g(\vec{w}, x_i) = \frac{-w^Tx_ie^{-w^Tx_i}}{1 + e^{-w^Tx_i}}
\end{equation}

Here, $\eta$ is a step size parameter that indicates how far to move the update in the direction of the gradient.

After learning the weight vector $w$, we can predict the label of a new vector $x$ by $\hat{y} = sgn(w^Tx)$. This prediction captures the relationship between $w$ and the conditional likelihood $p(y|x)$: when $p(y = 1|x) > \frac{1}{2}$, $w^Tx$ is positive, and vice versa.

\section{Related Work}
Several recent studies have also looked at parallelizing stochastic gradient descent. The Shotgun project parallelized $L_1$-regularized coordinate descent over features, proved near-linear speedup for their parallel algorithm, and empirically supported this theoretical result on several large datasets \cite{shotgun2011}. The Hogwild! project also theoretically proves near-linear speedup for sample-parallelized stochastic gradient descent under sparsity, even when the updates are made non-atomically: a lock-free approach does not often result in memory overwrites when the data are sparse because few parallel computations try to modify the same variable concurrently \cite{hogwild2011}. However, Hogwild! does assume that each the additive update to a component of the weight vector is an atomic operation; our implementation respects this atomicity. %Do we?

\section{Algorithms and Optimizations}
\subsection{Stochastic Gradient Descent}
In this project we implemented stochastic gradient descent for $L_2$ penalized logistic regression. The mathematical basis of this algorithm was explained in the Inroduction. Here we give pseudocode of the algorithms involved.
\hline
\hline
Algorithm 1
\hline
\begin{algorithmic}
\STATE{$w \leftarrow 0$}
\WHILE{!converged}
\FOR{$i = 0$ \TO $F-1$}
\FOR{$j = 0$ \TO $N-1$}
\STATE{$w[i] \leftarrow w[i] + \eta(y[j] - g(w[i], x[i, j]))$}
\ENDFOR
\STATE{$w[i] \leftarrow w[i] + \eta\lambda w[i]$}
\ENDFOR
\ENDWHILE
\end{algorithmic}
\hline
Algorithm 1 is the generic gradient descent. If the number of features is large, it can be very time consuming to update all features on each iteration of the algorithm, especially if it is likely that not all features are informative.  Thus, in stochastic gradient descent, a feature (or a set of features) is randomly chosen and its weight updated on each iteration using all or a subset of the training samples. This is shown in Algorithm 2.
\hline
\hline
Algorithm 2
\hline
\STATE{$w \leftarrow 0$}
\FOR{$t = 0$ \TO $num_iterations$}
\FOR{$i = 0$ \TO $batch_F$}
\STATE{choose $f$ randomly from $F$}
\FOR{$j = 0$ \TO $batch_N$}
\STATE{choose $n$ randomly from $N$}
\STATE{$w[f] \leftarrow w[f] + \eta(y[n] - g(w[f], x[f, n]))$}
\ENDFOR
\STATE{$w[f] \leftarrow w[f] + \eta\lambda w[f]$}
\ENDFOR
\ENDFOR
\hline
\subsection{Parallelized Versions}
There are several ways in which Algorithm 2 can be parallelized. In this project we explore two ways of parallelizing: batch updates, and a parallelization of those batch updates.  In each iteration, rather than updating the weight corresponding to one feature, which will require a large number of iterations to converge, we can update a subset of the weights (a batch).  If the probability that the same weight will appear more than once in a batch is low, then all the weights in the batch can be updated in parallel safely. We can also vary how many samples are used in each update.  Using too few will greatly increase the number of iterations needed to converge, but using all samples may not be necessary, either.  Updating a weight is simply adding an independent value to it.  Thus, we can parallelize the weight update across samples with minimal conflict in the result.

There are four levels of parallelization that we explored in this project:
\begin{enumerate}
\item
No parallelization
\item
Parallelization across features; updating all the weights in a batch in parallel
\item
Parallelization across samples; updating a weight with the samples in a batch in parallel
\item
Parallelization of both loops
\end{enumerate}
\subsection{Experiments}
Our goal was to see how the benefits gained by each level of parallelization change as the batch sizes are increased.  To that end, we ran Algorithm 2 at each level of parallelization, for varying batch sizes and compared the performance on a validation set as well as the runtime. The number of iterations and the step size were fixed throughout, and the regularization constant $\lambda$ was chosen with cross-validation of the training set for each feature and sample batch size pair. The only operation that was made atomic was the update over samples within an update of a particular weight (e.g. the innermost loop of Algorithm 2).
\subsubsection{Data Sets}
We used the arccene data set from among those used by \cite{shotgun2011}.

\section{Results}

% Explanation of what figures are what....
% {Runtime, Error} vs. {Feature Batch Size, Sample Batch Size}
%	- Each plot for four levels of parallelization
%	- (there are 16 plots here...might not include them all)
% {Runtime, Error} vs. {Feature Batch Size, Sample Batch Size}
%	- Each plot for both parallelizations: over features and samples
%	- (there are four plots here)

Figures~\cite{fig:NAME1} through \cite{fig:NAMEn} plot the runtimes and errors as the batch sizes (both features and samples) increase, for various levels of parallelization. As the batch sizes\textemdash both feature batch size and sample batch size\textemdash increase, there is the expected tradeoff between the convergence rate and runtime: when more features or samples are used to update the weights on each iteration, the algorithm converges more quickly, at the expense of more data processing and a longer runtime.

Of course, the fastest runtime and convergence, for given feature and sample batch sizes, occur when both levels of parallelization are employed. However, Figure~\cite{fig:NAME} indicates that performance improves more significantly when batch updates over features are parallelized than when batch updates over samples are parallelized. This result is likely due to the size of this data set: there are 100 times as many features as samples, so parallelizing over the more computationally-intensive portion of the data set would result in a bigger performance improvement.

\section{Conclusions}
The results demonstrate that while stochastic gradient descent involves many sequential passes through the data, parallelizing it leads to significant runtime improvements without adversely affecting the error convergence properties.  As the number of samples and/or features increases, it becomes more beneficial to parallelize across both dimensions.  The major benefit arises from parallelizing across features, because it allows multiple features to be updated during an iteration without significant runtime cost.Previous work demonstrated parallelizing either across samples or across features, but we have shown that parallelization is not only possible across both dimensions, but highly useful.

\bibliographystyle{ieeetr}
\bibliography{refs}

\end{document}
