\documentclass{article}
\usepackage{geometry}
%\usepackage{fancyhdr}
\usepackage{amsmath,amsthm,amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{enumerate}
\usepackage{caption}

\newcommand{\argmax}{\operatornamewithlimits{argmax}}

\title{Parallelizing Stochastic Gradient Descent}
\author{Kirstin Early \qquad Nicole Rafidi}
%\date{\today}

\begin{document}
\maketitle

\section{Introduction}
Gradient descent (also called coordinate descent) is a popular numerical technique for optimizing an expression that has no closed-form solution. This method can find a local minimum of a differentiable function by iteratively moving an initial guess of the minimum value in the opposite direction of the gradient at that point, either for a pre-specified number or iterations or until the update does not change the estimated value significantly. In the multivariate case, it is possible to update each coordinate individually in this manner. When data are numerous, stochastic gradient descent can improve efficiency by performing the update on a random subset of training examples or on a subset of features at each step.

In this project, we examine the performance improvement gained by parallelizing various sections of the stochastic gradient descent algorithm with OpenMP. We optimize the objective function for logistic regression with $L_2$ regularization:
\begin{equation}
\underset{w}{\min}-\frac{1}{1 + \exp(-y \cdot w^T x)} + \frac{\lambda}{2}||w||^2_2,
\end{equation}
where $x \in \mathbb{R}^F$ is a vector of sample features, $y \in \lbrace -1, 1 \rbrace$ is the sample label, and $w \in \mathbb{R}^F$ is a vector of feature weights. The first term in the optimization objective is the negative conditional likelihood of a sample's label given its features, $p(y|x)$; the second term is a regularization penalty on $w$. Sparse solutions (those solutions with few nonzero weights in $w$) are particularly desirable when the number of features exceeds the number of samples.

The optimum value for this objective function has no closed-form solution, and so we use gradient descent to find the minimum value. Letting $x \in \mathbb{R}^{N \times F}$ be the matrix of $N$ samples with $F$ features each, the update equation for a particular feature weight $w_f$ is
\begin{equation}
w_f = w_f + \eta \left( \sum_{i = 1}^{N}(y_i - g(\vec{w}, x_i)) + \lambda w_f \right)
\end{equation}
\begin{equation}
g(\vec{w}, x_i) = \frac{-w^Tx_ie^{-w^Tx_i}}{1 + e^{-w^Tx_i}}
\end{equation}

Here, $\eta$ is a step size parameter that indicates how far to move the update in the direction of the gradient.

After learning the weight vector $w$, we can predict the label of a new vector $x$ by $\hat{y} = sgn(w^Tx)$. This prediction captures the relationship between $w$ and the conditional likelihood $p(y|x)$: when $p(y = 1|x) > \frac{1}{2}$, $w^Tx$ is positive, and vice versa.

\section{Related Work}
Several recent studies have also looked at parallelizing stochastic gradient descent. The Shotgun project parallelized $L_1$-regularized coordinate descent over features, proved near-linear speedup for their parallel algorithm, and empirically supported this theoretical result on several large datasets \cite{shotgun2011}. The Hogwild! project also theoretically proves near-linear speedup for sample-parallelized stochastic gradient descent under sparsity, even when the updates are made non-atomically: a lock-free approach does not often result in memory overwrites when the data are sparse because few parallel computations try to modify the same variable concurrently \cite{hogwild2011}. However, Hogwild! does assume that each the additive update to a component of the weight vector is an atomic operation; our implementation respects this atomicity.

\section{Algorithms and Optimizations}
\subsection{Stochastic Gradient Descent}
In this project we implemented stochastic gradient descent for $L_2$ penalized logistic regression. The mathematical basis of this algorithm was explained in the Inroduction. Here we give pseudo
Algorithm 1 is the generic gradient descent (TODO:make algo fig). If the number of features is large, it can be very time consuming to update all features on each iteration of the algorithm, especially if it is likely that not all features are informative.  Thus, in stochastic gradient descent, a feature is randomly chosen and its weight updated on each iteration. This is shown in Algorithm 2 (TODO: make fig).
\subsection{Parallelized Versions}
There are several ways in which Algorithm 2 can be parallelized. In this project we explore two ways of parallelizing: batch updates, and a parallelization of those batch updates.  In each iteration, rather than updating the weight corresponding to one feature, which will require a large number of iterations to converge, we can update a subset of the weights (a batch).  If the probability that the same weight will appear more than once in a batch is low, then all the weights in the batch can be updated in parallel. The parallelized batch stochastic gradient descent is given in Algorithm 3 (TODO: make fig).
\subsection{Experiments}
The major experiment was to see how the benefits gained by parallelizing in Algorithm 3 change as the batch size is increased.  For that, we ran Algorithm 2 (with batch updates) and Algorithm 3, for varying batch sizes and compared the performance on a validation set as well as the runtime. The number of iterations and the step size were fixed for both, and the regularization constant $\lambda$ was chosen with cross-validation of the training set.
\subsubsection{Data Sets}
We used ? data sets from among those used by \cite{shotgun2011}.  The filenames are listed in Appendix whatever.

\section{Results}

% Explanation of what figures are what....
% {Runtime, Error} vs. {Feature Batch Size, Sample Batch Size}
%	- Each plot for four levels of parallelization
%	- (there are 16 plots here...might not include them all)
% {Runtime, Error} vs. {Feature Batch Size, Sample Batch Size}
%	- Each plot for both parallelizations: over features and samples
%	- (there are four plots here)

Figures~\cite{fig:NAME1} through \cite{fig:NAMEn} plot the runtimes and errors as the batch sizes (both features and samples) increase, for various levels of parallelization. As the batch sizes\textemdash both feature batch size and sample batch size\textemdash increase, there is the expected tradeoff between the convergence rate and runtime: when more features or samples are used to update the weights on each iteration, the algorithm converges more quickly, at the expense of more data processing and a longer runtime.

Of course, the fastest runtime and convergence, for given feature and sample batch sizes, occur when both levels of parallelization are employed. However, Figure~\cite{fig:NAME} indicates that performance improves more significantly when batch updates over features are parallelized than when batch updates over samples are parallelized. This result is likely due to the size of this data set: there are 100 times as many features as samples, so parallelizing over the more computationally-intensive portion of the data set would result in a bigger performance improvement.

\section{Conclusions}

\bibliographystyle{ieeetr}
\bibliography{refs}

\end{document}